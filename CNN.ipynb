{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3314.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2alCXo8--u6"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfiNuv5Tm3ur",
        "outputId": "60f3ecda-99e9-4c6b-a163-5d0b8c4e48e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ac_3Uc9tgq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision import transforms,datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCA7pXZI-HvJ",
        "outputId": "306090c2-6dee-4a7f-e9d5-588680e46271"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrv5GsY_DVBH"
      },
      "source": [
        "num_epochs = 10\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 16\n",
        "input_size = 32*32\n",
        "hidden_layers = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJDvM3amDp0X"
      },
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((32,32)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((32,32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Dataset initialization\n",
        "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']} # Read train and test sets, respectively.\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,shuffle=True, num_workers=2)for x in ['train', 'test']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
        "\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "trainLoader = dataloaders['train']\n",
        "testLoader = dataloaders['test']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjyqELTNForJ"
      },
      "source": [
        "class Net2(nn.Module):\n",
        "  def __init__(self, classes):\n",
        "    super(Net2, self).__init__()\n",
        "    self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "    self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "\n",
        "    self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "    self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "\n",
        "    self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "    self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "    self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "    self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "    self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "    self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "    self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "    self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "    self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "    # max pooling (kernel_size, stride)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    # fully conected layers:\n",
        "    self.fc6 = nn.Linear(512, 512)\n",
        "    self.fc7 = nn.Linear(512, 100)\n",
        "    self.fc8 = nn.Linear(100, 10)\n",
        "    self.fc9 = nn.Linear(1000,10)\n",
        "  def forward(self, x, training=True):\n",
        "    x = F.relu(self.conv1_1(x))\n",
        "    x = F.relu(self.conv1_2(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2_1(x))\n",
        "    x = F.relu(self.conv2_2(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv3_1(x))\n",
        "    x = F.relu(self.conv3_2(x))\n",
        "    x = F.relu(self.conv3_3(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv4_1(x))\n",
        "    x = F.relu(self.conv4_2(x))\n",
        "    x = F.relu(self.conv4_3(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv5_1(x))\n",
        "    x = F.relu(self.conv5_2(x))\n",
        "    x = F.relu(self.conv5_3(x))\n",
        "    x = self.pool(x)\n",
        "    x = x.view(-1, 512)\n",
        "    x = F.relu(self.fc6(x))\n",
        "    x = F.dropout(x, 0.5, training=training)\n",
        "    x = F.relu(self.fc7(x))\n",
        "    x = F.dropout(x, 0.5, training=training)\n",
        "    x = self.fc8(x)\n",
        "    return x\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGH0yiYkFefo"
      },
      "source": [
        "class Net1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(3, 256, 3),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(256, 128, 2),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(128, 64, 2),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(64, 32, 2),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(32, 8, 2),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Conv2d(8,8,1),\n",
        "            nn.Conv2d(8,4,1),\n",
        "            nn.Conv2d(4,4,1),\n",
        "            nn.Conv2d(4,2,1),  \n",
        "            nn.Conv2d(2,2,1),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(338, 64),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(64,20),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(20, 10),\n",
        "            \n",
        "        )\n",
        "      \n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.convs(img)\n",
        "       # print(x.shape)\n",
        "        x = x.view(4,338)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCwWOOPZHRfh"
      },
      "source": [
        "def train_test(model, criterion, optimizer, scheduler, num_epochs=1500):\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss=0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainLoader,0):\n",
        "      #print(type(data))\n",
        "      inputs, labels = data\n",
        "     # inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      running_loss+=loss.item()\n",
        "    #print(epoch) \n",
        "      \n",
        "      _, predicted = torch.max(outputs.data,1)\n",
        "      total+=labels.size(0)\n",
        "      correct+= (predicted==labels).sum().item()\n",
        "      if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss /2000 ))\n",
        "        running_loss = 0.0\n",
        "        print('Accuracy: ', 100*correct/total)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "       # for i,para in enumerate(model.parameters()):\n",
        "        #  print(f'{i+1}th parameter tensor:')\n",
        "          #print(para)\n",
        "        #  print(para.grad)\n",
        "        #print()\n",
        "    #  print(\"Epoch: {}/{}. step: {}/{}, loss: {:.4f}\".format(epoch, num_epochs, i, len(trainLoader), loss.item()))\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for data in testLoader:\n",
        "        images, labels = data\n",
        "       # images, labels = images.cuda(), labels.cuda()\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data,1)\n",
        "        total+=labels.size(0)\n",
        "        correct+= (predicted==labels).sum().item()\n",
        "    print('Accuracy: ', 100*correct/total)\n",
        "    \n",
        "  return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdJx349UOvME"
      },
      "source": [
        "class Net3(nn.Module):\n",
        "    \"\"\"\n",
        "    Input - 1x32x32\n",
        "    Output - 10\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Net3, self).__init__()\n",
        "        # TODO: Initialize layers\n",
        "        self.conv= nn.Sequential(\n",
        "          nn.Conv2d(3,32,3),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(32,128,2),\n",
        "          nn.BatchNorm2d(128),\n",
        "          nn.MaxPool2d(3, 2),\n",
        "          nn.ReLU(),\n",
        "          nn.Flatten()\n",
        "        )\n",
        "\n",
        "\n",
        "        flatten_size = self.get_flatten_size([4,3,32,32])\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(flatten_size,32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32,10),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "    def get_flatten_size(self,shape):\n",
        "        dummy_img = torch.zeros(size=shape)\n",
        "        emb_1 = self.conv(dummy_img)\n",
        "        #conv_fc = self.nin_block(emb_1)\n",
        "        return int(np.prod(emb_1.shape[1:]))\n",
        "\n",
        "    def forward(self, img):\n",
        "\n",
        "        # TODO: Implement forward pass\n",
        "        emb_1 = self.conv(img)\n",
        "        # conv_fc = self.nin_block(emb_1)\n",
        "        y = self.fc(emb_1)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3IhNYYeIMvp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c43d2f0-690b-4c7a-d115-d6698eaf8490"
      },
      "source": [
        "model_ft = Net3() # Model initialization\n",
        "\n",
        "model_ft = model_ft.to(device) # Move model to cpu\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
        "\n",
        "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.0001) # Optimizer initialization\n",
        "#optimizer_ft = optim.Adam(lr=0.001)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
        "\n",
        "train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=1500)\n",
        "\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.316\n",
            "Accuracy:  10.475\n",
            "[1,  4000] loss: 2.316\n",
            "Accuracy:  10.175\n",
            "[1,  6000] loss: 2.320\n",
            "Accuracy:  9.8375\n",
            "Accuracy:  10.14\n",
            "[2,  2000] loss: 2.315\n",
            "Accuracy:  10.4625\n",
            "[2,  4000] loss: 2.317\n",
            "Accuracy:  10.4125\n",
            "[2,  6000] loss: 2.315\n",
            "Accuracy:  10.725\n",
            "Accuracy:  10.6\n",
            "[3,  2000] loss: 2.317\n",
            "Accuracy:  10.55\n",
            "[3,  4000] loss: 2.317\n",
            "Accuracy:  10.5375\n",
            "[3,  6000] loss: 2.315\n",
            "Accuracy:  10.6375\n",
            "Accuracy:  9.94\n",
            "[4,  2000] loss: 2.316\n",
            "Accuracy:  10.2625\n",
            "[4,  4000] loss: 2.319\n",
            "Accuracy:  10.1875\n",
            "[4,  6000] loss: 2.315\n",
            "Accuracy:  10.775\n",
            "Accuracy:  9.78\n",
            "[5,  2000] loss: 2.314\n",
            "Accuracy:  10.6875\n",
            "[5,  4000] loss: 2.318\n",
            "Accuracy:  10.15\n",
            "[5,  6000] loss: 2.316\n",
            "Accuracy:  10.4875\n",
            "Accuracy:  9.86\n",
            "[6,  2000] loss: 2.315\n",
            "Accuracy:  10.7375\n",
            "[6,  4000] loss: 2.318\n",
            "Accuracy:  10.425\n",
            "[6,  6000] loss: 2.317\n",
            "Accuracy:  10.25\n",
            "Accuracy:  10.12\n",
            "[7,  2000] loss: 2.317\n",
            "Accuracy:  10.375\n",
            "[7,  4000] loss: 2.317\n",
            "Accuracy:  9.9\n",
            "[7,  6000] loss: 2.317\n",
            "Accuracy:  10.5\n",
            "Accuracy:  10.4\n",
            "[8,  2000] loss: 2.314\n",
            "Accuracy:  10.7875\n",
            "[8,  4000] loss: 2.317\n",
            "Accuracy:  10.3125\n",
            "[8,  6000] loss: 2.318\n",
            "Accuracy:  10.225\n",
            "Accuracy:  10.0\n",
            "[9,  2000] loss: 2.315\n",
            "Accuracy:  10.4375\n",
            "[9,  4000] loss: 2.319\n",
            "Accuracy:  10.0125\n",
            "[9,  6000] loss: 2.317\n",
            "Accuracy:  10.1\n",
            "Accuracy:  10.2\n",
            "[10,  2000] loss: 2.314\n",
            "Accuracy:  10.7375\n",
            "[10,  4000] loss: 2.316\n",
            "Accuracy:  10.4625\n",
            "[10,  6000] loss: 2.318\n",
            "Accuracy:  10.2125\n",
            "Accuracy:  10.34\n",
            "[11,  2000] loss: 2.312\n",
            "Accuracy:  10.7125\n",
            "[11,  4000] loss: 2.315\n",
            "Accuracy:  10.9125\n",
            "[11,  6000] loss: 2.322\n",
            "Accuracy:  9.8375\n",
            "Accuracy:  10.32\n",
            "[12,  2000] loss: 2.316\n",
            "Accuracy:  10.3375\n",
            "[12,  4000] loss: 2.318\n",
            "Accuracy:  10.3125\n",
            "[12,  6000] loss: 2.315\n",
            "Accuracy:  10.675\n",
            "Accuracy:  10.04\n",
            "[13,  2000] loss: 2.315\n",
            "Accuracy:  10.7625\n",
            "[13,  4000] loss: 2.318\n",
            "Accuracy:  9.85\n",
            "[13,  6000] loss: 2.314\n",
            "Accuracy:  11.0875\n",
            "Accuracy:  10.04\n",
            "[14,  2000] loss: 2.312\n",
            "Accuracy:  10.9375\n",
            "[14,  4000] loss: 2.317\n",
            "Accuracy:  10.2875\n",
            "[14,  6000] loss: 2.315\n",
            "Accuracy:  10.55\n",
            "Accuracy:  10.1\n",
            "[15,  2000] loss: 2.314\n",
            "Accuracy:  10.875\n",
            "[15,  4000] loss: 2.318\n",
            "Accuracy:  10.225\n",
            "[15,  6000] loss: 2.315\n",
            "Accuracy:  10.3625\n",
            "Accuracy:  10.24\n",
            "[16,  2000] loss: 2.317\n",
            "Accuracy:  10.075\n",
            "[16,  4000] loss: 2.317\n",
            "Accuracy:  10.2375\n",
            "[16,  6000] loss: 2.317\n",
            "Accuracy:  10.5625\n",
            "Accuracy:  9.88\n",
            "[17,  2000] loss: 2.315\n",
            "Accuracy:  10.5625\n",
            "[17,  4000] loss: 2.316\n",
            "Accuracy:  10.625\n",
            "[17,  6000] loss: 2.318\n",
            "Accuracy:  10.325\n",
            "Accuracy:  10.2\n",
            "[18,  2000] loss: 2.315\n",
            "Accuracy:  10.7125\n",
            "[18,  4000] loss: 2.316\n",
            "Accuracy:  10.65\n",
            "[18,  6000] loss: 2.316\n",
            "Accuracy:  10.225\n",
            "Accuracy:  10.22\n",
            "[19,  2000] loss: 2.320\n",
            "Accuracy:  9.9125\n",
            "[19,  4000] loss: 2.316\n",
            "Accuracy:  10.5625\n",
            "[19,  6000] loss: 2.315\n",
            "Accuracy:  10.5625\n",
            "Accuracy:  10.06\n",
            "[20,  2000] loss: 2.316\n",
            "Accuracy:  10.375\n",
            "[20,  4000] loss: 2.317\n",
            "Accuracy:  10.25\n",
            "[20,  6000] loss: 2.316\n",
            "Accuracy:  10.4375\n",
            "Accuracy:  10.24\n",
            "[21,  2000] loss: 2.317\n",
            "Accuracy:  10.1125\n",
            "[21,  4000] loss: 2.317\n",
            "Accuracy:  10.4\n",
            "[21,  6000] loss: 2.316\n",
            "Accuracy:  10.4375\n",
            "Accuracy:  10.42\n",
            "[22,  2000] loss: 2.314\n",
            "Accuracy:  10.575\n",
            "[22,  4000] loss: 2.316\n",
            "Accuracy:  10.15\n",
            "[22,  6000] loss: 2.318\n",
            "Accuracy:  10.25\n",
            "Accuracy:  10.2\n",
            "[23,  2000] loss: 2.316\n",
            "Accuracy:  9.9375\n",
            "[23,  4000] loss: 2.318\n",
            "Accuracy:  10.2875\n",
            "[23,  6000] loss: 2.315\n",
            "Accuracy:  10.4875\n",
            "Accuracy:  10.02\n",
            "[24,  2000] loss: 2.315\n",
            "Accuracy:  10.65\n",
            "[24,  4000] loss: 2.315\n",
            "Accuracy:  10.5625\n",
            "[24,  6000] loss: 2.319\n",
            "Accuracy:  10.0375\n",
            "Accuracy:  10.54\n",
            "[25,  2000] loss: 2.316\n",
            "Accuracy:  10.4625\n",
            "[25,  4000] loss: 2.317\n",
            "Accuracy:  10.0625\n",
            "[25,  6000] loss: 2.317\n",
            "Accuracy:  10.35\n",
            "Accuracy:  10.32\n",
            "[26,  2000] loss: 2.316\n",
            "Accuracy:  10.2375\n",
            "[26,  4000] loss: 2.317\n",
            "Accuracy:  10.225\n",
            "[26,  6000] loss: 2.316\n",
            "Accuracy:  10.5375\n",
            "Accuracy:  10.2\n",
            "[27,  2000] loss: 2.318\n",
            "Accuracy:  10.3\n",
            "[27,  4000] loss: 2.316\n",
            "Accuracy:  10.3875\n",
            "[27,  6000] loss: 2.318\n",
            "Accuracy:  10.15\n",
            "Accuracy:  10.16\n",
            "[28,  2000] loss: 2.316\n",
            "Accuracy:  10.6125\n",
            "[28,  4000] loss: 2.319\n",
            "Accuracy:  10.0625\n",
            "[28,  6000] loss: 2.313\n",
            "Accuracy:  10.9\n",
            "Accuracy:  10.12\n",
            "[29,  2000] loss: 2.315\n",
            "Accuracy:  10.4875\n",
            "[29,  4000] loss: 2.316\n",
            "Accuracy:  10.375\n",
            "[29,  6000] loss: 2.317\n",
            "Accuracy:  10.6125\n",
            "Accuracy:  10.04\n",
            "[30,  2000] loss: 2.316\n",
            "Accuracy:  10.575\n",
            "[30,  4000] loss: 2.316\n",
            "Accuracy:  10.4625\n",
            "[30,  6000] loss: 2.316\n",
            "Accuracy:  10.5875\n",
            "Accuracy:  10.34\n",
            "[31,  2000] loss: 2.318\n",
            "Accuracy:  10.075\n",
            "[31,  4000] loss: 2.316\n",
            "Accuracy:  10.3125\n",
            "[31,  6000] loss: 2.316\n",
            "Accuracy:  10.625\n",
            "Accuracy:  10.2\n",
            "[32,  2000] loss: 2.317\n",
            "Accuracy:  10.225\n",
            "[32,  4000] loss: 2.316\n",
            "Accuracy:  10.375\n",
            "[32,  6000] loss: 2.317\n",
            "Accuracy:  10.1875\n",
            "Accuracy:  10.44\n",
            "[33,  2000] loss: 2.318\n",
            "Accuracy:  10.4\n",
            "[33,  4000] loss: 2.315\n",
            "Accuracy:  10.8375\n",
            "[33,  6000] loss: 2.317\n",
            "Accuracy:  10.25\n",
            "Accuracy:  10.42\n",
            "[34,  2000] loss: 2.315\n",
            "Accuracy:  10.375\n",
            "[34,  4000] loss: 2.318\n",
            "Accuracy:  9.9625\n",
            "[34,  6000] loss: 2.314\n",
            "Accuracy:  10.9875\n",
            "Accuracy:  10.22\n",
            "[35,  2000] loss: 2.318\n",
            "Accuracy:  9.95\n",
            "[35,  4000] loss: 2.318\n",
            "Accuracy:  10.2375\n",
            "[35,  6000] loss: 2.316\n",
            "Accuracy:  10.55\n",
            "Accuracy:  10.2\n",
            "[36,  2000] loss: 2.318\n",
            "Accuracy:  9.8875\n",
            "[36,  4000] loss: 2.313\n",
            "Accuracy:  10.7375\n",
            "[36,  6000] loss: 2.316\n",
            "Accuracy:  10.6375\n",
            "Accuracy:  10.48\n",
            "[37,  2000] loss: 2.315\n",
            "Accuracy:  10.6125\n",
            "[37,  4000] loss: 2.316\n",
            "Accuracy:  10.7\n",
            "[37,  6000] loss: 2.317\n",
            "Accuracy:  10.4125\n",
            "Accuracy:  10.1\n",
            "[38,  2000] loss: 2.316\n",
            "Accuracy:  10.3625\n",
            "[38,  4000] loss: 2.316\n",
            "Accuracy:  10.5125\n",
            "[38,  6000] loss: 2.315\n",
            "Accuracy:  10.4875\n",
            "Accuracy:  10.5\n",
            "[39,  2000] loss: 2.318\n",
            "Accuracy:  9.975\n",
            "[39,  4000] loss: 2.315\n",
            "Accuracy:  10.8125\n",
            "[39,  6000] loss: 2.319\n",
            "Accuracy:  9.9375\n",
            "Accuracy:  10.32\n",
            "[40,  2000] loss: 2.317\n",
            "Accuracy:  10.075\n",
            "[40,  4000] loss: 2.319\n",
            "Accuracy:  10.2\n",
            "[40,  6000] loss: 2.319\n",
            "Accuracy:  9.9125\n",
            "Accuracy:  10.54\n",
            "[41,  2000] loss: 2.314\n",
            "Accuracy:  11.0125\n",
            "[41,  4000] loss: 2.314\n",
            "Accuracy:  10.425\n",
            "[41,  6000] loss: 2.320\n",
            "Accuracy:  9.95\n",
            "Accuracy:  10.38\n",
            "[42,  2000] loss: 2.314\n",
            "Accuracy:  10.7875\n",
            "[42,  4000] loss: 2.319\n",
            "Accuracy:  10.1\n",
            "[42,  6000] loss: 2.314\n",
            "Accuracy:  10.8\n",
            "Accuracy:  10.38\n",
            "[43,  2000] loss: 2.317\n",
            "Accuracy:  10.425\n",
            "[43,  4000] loss: 2.320\n",
            "Accuracy:  10.3\n",
            "[43,  6000] loss: 2.314\n",
            "Accuracy:  10.725\n",
            "Accuracy:  10.06\n",
            "[44,  2000] loss: 2.318\n",
            "Accuracy:  9.9625\n",
            "[44,  4000] loss: 2.316\n",
            "Accuracy:  10.4625\n",
            "[44,  6000] loss: 2.316\n",
            "Accuracy:  10.4375\n",
            "Accuracy:  10.22\n",
            "[45,  2000] loss: 2.314\n",
            "Accuracy:  10.5375\n",
            "[45,  4000] loss: 2.315\n",
            "Accuracy:  10.8375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f4b12d039dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Learning rate decay strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/MyDrive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-9af5cfdf341e>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx6LnIHbjgQp"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net6(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(Net6, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(9216, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(4,9216)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m9a-_UkYYyD"
      },
      "source": [
        "class Net4(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Net4, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.fc = nn.Linear(in_features=16 * 16 * 24, out_features=num_classes)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv1(input)\n",
        "        output = self.relu1(output)\n",
        "\n",
        "        output = self.conv2(output)\n",
        "        output = self.relu2(output)\n",
        "\n",
        "        output = self.pool(output)\n",
        "\n",
        "        output = self.conv3(output)\n",
        "        output = self.relu3(output)\n",
        "\n",
        "        output = self.conv4(output)\n",
        "        output = self.relu4(output)\n",
        "\n",
        "        output = output.view(-1, 16 * 16 * 24)\n",
        "\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlPqSLq_nwqi"
      },
      "source": [
        "class Net7(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net7, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 128, 3)\n",
        "    self.conv2 = nn.Conv2d(128,)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}